{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tabula-py\n",
        "!pip install PyPDF2\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install PyPDF2\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install --upgrade openai\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "EWLtRr0t40a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "import tabula"
      ],
      "metadata": {
        "id": "YUAma07Q-st5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "8AAvFXDL-3I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_openai_api_key():\n",
        "    api_key = input(\"Enter your OpenAI API key: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "     # Use PyMuPDF to extract text from the PDF\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as pdf_document:\n",
        "        for page_num in range(pdf_document.page_count):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_table_data(pdf_path):\n",
        "    # Use tabula-py to extract tables from the PDF\n",
        "    tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
        "    table_data = []\n",
        "\n",
        "    for table in tables:\n",
        "        # Convert the table data to a list of dictionaries for easier access\n",
        "        table_data = table.to_dict(orient=\"records\")\n",
        "\n",
        "        # Alternate way: Use list comprehension for concise code\n",
        "        formatted_rows = [\", \".join(map(str, row.values())) for row in table_data]\n",
        "\n",
        "        # Join the formatted rows with newline characters\n",
        "        table_text = \"\\n\".join(formatted_rows)\n",
        "        table_data.append(table_text)\n",
        "\n",
        "    return table_data"
      ],
      "metadata": {
        "id": "XtRDC_gB-umm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Upload a PDF file\n",
        "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
        "    pdf_name = os.path.basename(pdf_path)\n",
        "\n",
        "   # Extract text from the PDF using PyMuPDF\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Preprocess the text to handle any encoding issues or anomalies\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    text_chunks = text_splitter.split_text(text=text)\n",
        "\n",
        "    # Extract tables from the PDF using tabula-py\n",
        "    table_data = extract_table_data(pdf_path)\n",
        "\n",
        "    # Create vector store for text chunks\n",
        "    text_store_name = pdf_name[:-4] + \"_text\"\n",
        "    if os.path.exists(f\"{text_store_name}.pkl\"):\n",
        "        with open(f\"{text_store_name}.pkl\", \"rb\") as f:\n",
        "            TextVectorStore = pickle.load(f)\n",
        "        print(\"Text embeddings loaded from disk\")\n",
        "    else:\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        TextVectorStore = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "        with open(f\"{text_store_name}.pkl\", \"wb\") as f:\n",
        "            pickle.dump(TextVectorStore, f)\n",
        "        print(\"Text embeddings created and saved to disk.\")\n",
        "\n",
        "    # Create vector store for table data\n",
        "    table_store_name = pdf_name[:-4] + \"_tables\"\n",
        "    if os.path.exists(f\"{table_store_name}.pkl\"):\n",
        "        with open(f\"{table_store_name}.pkl\", \"rb\") as f:\n",
        "            TableVectorStore = pickle.load(f)\n",
        "        print(\"Table embeddings loaded from disk.\")\n",
        "    else:\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        TableVectorStore = FAISS.from_texts(table_data, embedding=embeddings)\n",
        "        with open(f\"{table_store_name}.pkl\", \"wb\") as f:\n",
        "            pickle.dump(TableVectorStore, f)\n",
        "        print(\"Table embeddings created and saved to disk.\")\n",
        "\n",
        "    # Initialize conversation history\n",
        "    conversation = []\n",
        "\n",
        "    # Accept user prompts (questions)\n",
        "    while True:\n",
        "        query = input(\"Ask a question about the PDF file (or type 'exit' to quit): \")\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "  # Append the current query to conversation history\n",
        "        conversation.append(query)\n",
        "\n",
        "        # Configure OpenAI LLM parameters\n",
        "        llm = OpenAI(temperature=0, model_name=\"gpt-4\")\n",
        "\n",
        "        # Combine the entire conversation history as context for better understanding\n",
        "        conversation_context = \"\\n\".join(conversation)\n",
        "\n",
        "        # Perform a similarity search between the user's question and the text chunks\n",
        "        text_docs = TextVectorStore.similarity_search(query=conversation_context, k=3)\n",
        "\n",
        "        # Perform similarity search between query and table data\n",
        "        table_docs = TableVectorStore.similarity_search(query=conversation_context, k=3)\n",
        "\n",
        "        # Combine the results from text and table searches\n",
        "        all_docs = text_docs + table_docs\n",
        "\n",
        "        chain = load_qa_chain(llm=llm, chain_type=\"stuff\")\n",
        "        with get_openai_callback() as cb:\n",
        "            response = chain.run(input_documents=all_docs, question=query)\n",
        "            tokens = cb.total_tokens\n",
        "            cost = cb.total_cost\n",
        "\n",
        "   # Append the response to conversation history\n",
        "        conversation.append(response)\n",
        "\n",
        "        print(\"OpenAI API Cost:\", cost)\n",
        "        print(\"Number of Tokens Used:\", tokens)\n",
        "        print(\"Response:\",response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    setup_openai_api_key()\n",
        "    main()"
      ],
      "metadata": {
        "id": "rbJfjrcj-zQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}