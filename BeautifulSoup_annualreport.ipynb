{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8vgeP4WIJdF"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as soup\n",
        "import requests as r\n",
        "import pandas\n",
        "import time\n",
        "import os\n",
        "import datetime\n",
        "import random\n",
        "import shutil\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_input_file(input_path, raw_filename, raw_sheetname):\n",
        "    \"\"\" Create pandas dataframe out of Excel file for scraping below\n",
        "    Args:\n",
        "        input path\n",
        "        file name of the Excel file in the directory of consideration\n",
        "        sheet name of the Excel file\n",
        "    Returns:\n",
        "        pandas dataframe\n",
        "    \"\"\"\n",
        "    fpath = input_path + raw_filename\n",
        "    return pandas.read_excel(fpath, sheet_name  = raw_sheetname)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NhOvw9WIJo2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_firm_list(file, firms):\n",
        "    \"\"\" Creates a list of firms for PDF download\n",
        "    Args:\n",
        "        Either several firms (comma-seperated list) or a '-' for all S&P500.\n",
        "    Returns:\n",
        "        A list of firms for parsing.\n",
        "    \"\"\"\n",
        "    # 1. Check correctness of zip code inputs\n",
        "    for i in range(0,len(firms)):\n",
        "        if len(firms[0]) == 1 and len(firms) == 1 and firms[0] == \"-\":\n",
        "            return file['Symbol'].values.tolist()\n",
        "        elif len(firms[0]) != 1:\n",
        "            return firms\n",
        "\n"
      ],
      "metadata": {
        "id": "UgMsYil2JrWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reveal_true_firm_name(file, firm):\n",
        "    \"\"\"Reveals true frim name upon enterig code\n",
        "    Args:\n",
        "        Firm code\n",
        "        Input file with mapping\n",
        "    Returns:\n",
        "        Company name\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return file[file.Symbol == firm].Company.values.tolist()[0]\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "def select_first_letter(firm, file):\n",
        "    \"\"\" Select first letter from firm in loop to construct listings URL.\n",
        "    Args:\n",
        "        firm abbreviation\n",
        "        input file with firm name and firm code\n",
        "    Returns:\n",
        "        First letter of firm in lower case\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return file[file.Symbol == firm].Company.values.tolist()[0][0].lower()\n",
        "    except (KeyError, IndexError):\n",
        "        return 'a'\n",
        "\n",
        "def set_url(first_letter, stock_exchange, firm ,year):\n",
        "    \"\"\" Constructs URL for extracting PDF document\n",
        "    Args:\n",
        "        First letter of firm\n",
        "        Name of stock exchange (e.g. NYSE or NASDAQ)\n",
        "        Firm code\n",
        "        Year of annual report\n",
        "    Returns:\n",
        "        Callable URL for http get request\n",
        "    \"\"\"\n",
        "    if year == 2022:\n",
        "        return \"http://annualreports.com/HostedData/AnnualReports/\" + \"PDF/\" + stock_exchange + \"_\" + firm + \"_\" + str(year) + \".pdf\"\n",
        "    else:\n",
        "        return \"http://annualreports.com/HostedData/AnnualReportArchive/\" + first_letter + \"/\" + stock_exchange + \"_\" + firm + \"_\" + str(year) + \".pdf\"\n",
        "\n",
        "def adjust_firm_list(firm, firms):\n",
        "    \"\"\" Adjusts firms list to restart properly after crash\n",
        "    Args:\n",
        "        Current firm (string)\n",
        "        All firms (list)\n",
        "    Returns:\n",
        "        Jumps to page where last stopped\n",
        "    \"\"\"\n",
        "    return firms[firms.index(firm):len(firms)]\n",
        "\n",
        "def clean_firm_name(true_firm_name):\n",
        "    \"\"\" Cleans special characters from firm name to facilitate creation\n",
        "    of directory.\n",
        "    Ars:\n",
        "        True firm name\n",
        "    Returns:\n",
        "        Cleaned firm name\n",
        "    \"\"\"\n",
        "    symbols = ['!', '\"', '#', '$', '%', \"'\", '(', ')', '*', '+', ',', '-',\n",
        "               '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '`', '{',\n",
        "               '|', '}', '~']\n",
        "    for symbol in symbols:\n",
        "        true_firm_name = true_firm_name.replace(symbol, '_')\n",
        "    return true_firm_name\n",
        "\n",
        "def scrape_annualreports(file, firms, stock_exchanges, start_year, end_year, input_path,\n",
        "                         raw_filename, raw_sheetname, output_path, now_str, max_repeats):\n",
        "    \"\"\" Scrape annual reports from annualreports.com\n",
        "    Args:\n",
        "        input file with firm name and firm code\n",
        "        firm abbreviation\n",
        "        Name of stock exchange (e.g. NYSE or NASDAQ)\n",
        "    Returns:\n",
        "        Saves PDF documents in indicated folder, creating folder with scraping\n",
        "        time firm code and year\n",
        "    \"\"\"\n",
        "    on_repeat = False\n",
        "    first_run = True\n",
        "    counter = 0\n",
        "    while on_repeat or first_run:\n",
        "        counter += 1\n",
        "        if counter >= max_repeats:\n",
        "            break\n",
        "        print(\"Running iteration\", counter, \"of parser ...\")\n",
        "        try:\n",
        "            # Set firms list\n",
        "            firms = set_firm_list(file, firms)\n",
        "            for firm in firms:\n",
        "                # Reveal true name\n",
        "                true_firm = clean_firm_name(reveal_true_firm_name(file, firm))\n",
        "                # Shorten firm list in case of crash\n",
        "                firms = adjust_firm_list(firm, firms)\n",
        "                print(\"Parsing annual reports of\",true_firm,\"...\")\n",
        "                # Create subfolder for firm of consideration\n",
        "                firm_folder_name = output_path + now_str + \"/\" + true_firm + \"/\"\n",
        "                os.mkdir(firm_folder_name)\n",
        "                for stock_exchange in stock_exchanges:\n",
        "                    for year in range(start_year, end_year+1):\n",
        "                        # Timeout\n",
        "                        time.sleep(random.randint(1,3))\n",
        "                        # Define URL\n",
        "                        url = set_url(select_first_letter(firm, file), stock_exchange, firm ,year)\n",
        "                        # Get response code\n",
        "                        response = r.get(url, timeout = 60)\n",
        "                        # Extract file dpending on the response\n",
        "                        if response.status_code == 200:\n",
        "                            outfile = firm_folder_name  + firm + \"_\" + str(year) + \".pdf\"\n",
        "                            with open(outfile, 'wb') as f:\n",
        "                                first_run = False\n",
        "                                on_repeat = False\n",
        "                                f.write(response.content)\n",
        "                        else:\n",
        "                            continue\n",
        "        except r.exceptions.ConnectionError:\n",
        "            print(\"Connection was interrupted, waiting a few moments before continuing...\")\n",
        "            time.sleep(random.randint(2,5) + counter)\n",
        "            on_repeat = True\n",
        "            # Delete folder and restart\n",
        "            shutil.rmtree(firm_folder_name, ignore_errors=True)\n",
        "            continue\n",
        "        except TypeError:\n",
        "            print(\"Error encountered, skipping firm ...\")\n",
        "            continue\n",
        "\n"
      ],
      "metadata": {
        "id": "bvbKCAo7JxOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    # Capture start and end time for performance\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create folder for current scrape\n",
        "    # Set now string\n",
        "    now_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Set output path\n",
        "    input_path = \"/content/drive/MyDrive/annual_report\"\n",
        "    output_path = \"/content/drive/MyDrive/annual_report\"\n",
        "\n",
        "    # Set maximum repeats before crash\n",
        "    max_repeats = 30\n",
        "\n",
        "    # Create folder for listing output files\n",
        "    time_folder = output_path + now_str\n",
        "    os.mkdir(time_folder)\n",
        "    # Import file with firm level codes\n",
        "    raw_filename = \"Sampling_S&P_500.xlsx\"\n",
        "    raw_sheetname = \"S&P500_sampling\"\n",
        "    file = create_input_file(\"/content/\", \"Sampling_S&P_500.xlsx\", \"S&P500_data\")\n",
        "\n",
        "    # Set start and end year of consideration\n",
        "    start_year = 2017\n",
        "\n",
        "    end_year = 2022\n",
        "\n",
        "    # Set firms to be scraped (either '-' for all S&P500 or Firm code from list)\n",
        "    firms = ['-']\n",
        "\n",
        "    # Set names of stock exchanges to be read\n",
        "    stock_exchanges = ['NYSE', 'NASDAQ']\n",
        "\n",
        "    # Run scraper\n",
        "    scrape_annualreports(file, firms, stock_exchanges, start_year, end_year, input_path, raw_filename, raw_sheetname, output_path, now_str, max_repeats)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = time.strftime(\"%H:%M:%S\", time.gmtime(end_time - start_time))\n",
        "\n",
        "    final_text = \"Your query was successful! Time elapsed:\" + str(duration)\n",
        "    print(final_text)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "\n",
        "# Execute scraping\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j605aMrJ2Gg",
        "outputId": "820df6d8-8b6e-488e-cef9-cef95d545358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running iteration 1 of parser ...\n",
            "Parsing annual reports of Bank of America Corporation ...\n",
            "Parsing annual reports of Baker Hughes ...\n",
            "Parsing annual reports of Bio_Rad ...\n",
            "Parsing annual reports of BIO_TECHNE CORPORATION ...\n",
            "Parsing annual reports of Broadcom inc_ ...\n",
            "Parsing annual reports of Broadridge Financial Solutions ...\n",
            "Parsing annual reports of Brown & Brown ...\n",
            "Parsing annual reports of Bunge Limited ...\n",
            "Parsing annual reports of Baxter International Inc_ ...\n",
            "Parsing annual reports of Truist Financial ...\n",
            "Parsing annual reports of Best Buy Co__ Inc_ ...\n",
            "Parsing annual reports of Becton_ Dickinson and Company ...\n",
            "Parsing annual reports of Franklin Resources Inc_ ...\n",
            "Parsing annual reports of Brown_Forman Corporation ...\n",
            "Parsing annual reports of Biogen Idec Inc_ ...\n",
            "Parsing annual reports of The Bank of New York Mellon Corporation ...\n",
            "Parsing annual reports of BlackRock_ Inc_ ...\n",
            "Parsing annual reports of Ball Corporation ...\n",
            "Parsing annual reports of Bristol_Myers Squibb Company ...\n",
            "Parsing annual reports of Berkshire Hathaway Inc_ ...\n",
            "Parsing annual reports of Boston Scientific Corporation ...\n",
            "Parsing annual reports of BorgWarner Inc_ ...\n",
            "Parsing annual reports of Boston Properties Inc_ ...\n",
            "Parsing annual reports of Citigroup Inc_ ...\n",
            "Parsing annual reports of ConAgra Foods_ Inc_ ...\n",
            "Parsing annual reports of Cardinal Health_ Inc_ ...\n",
            "Parsing annual reports of Caterpillar Inc_ ...\n",
            "Parsing annual reports of CBRE Group_ Inc_ ...\n",
            "Parsing annual reports of Crown Castle International Corp_ ...\n",
            "Parsing annual reports of Carnival Corporation ...\n",
            "Parsing annual reports of CF Industries Holdings_ Inc_ ...\n",
            "Parsing annual reports of CH Robinson Worldwide Inc_ ...\n",
            "Parsing annual reports of Cincinnati Financial Corp_ ...\n",
            "Parsing annual reports of Colgate_Palmolive Co_ ...\n",
            "Parsing annual reports of The Clorox Company ...\n",
            "Parsing annual reports of Comerica Incorporated ...\n",
            "Parsing annual reports of Comcast Corporation ...\n",
            "Parsing annual reports of CME Group Inc_ ...\n",
            "Parsing annual reports of Chipotle Mexican Grill_ Inc_ ...\n",
            "Parsing annual reports of Cummins Inc_ ...\n",
            "Parsing annual reports of CMS Energy Corp_ ...\n",
            "Parsing annual reports of CenterPoint Energy_ Inc_ ...\n",
            "Parsing annual reports of Capital One Financial Corporation ...\n",
            "Parsing annual reports of Cabot Oil & Gas Corporation ...\n",
            "Parsing annual reports of Tapestry_ Inc_ ...\n",
            "Parsing annual reports of ConocoPhillips ...\n",
            "Parsing annual reports of Costco Wholesale Corporation ...\n",
            "Parsing annual reports of Campbell Soup Company ...\n",
            "Parsing annual reports of Salesforce_com_ Inc ...\n",
            "Parsing annual reports of Cisco Systems_ Inc_ ...\n",
            "Parsing annual reports of CSX Corp_ ...\n",
            "Parsing annual reports of Cintas Corporation ...\n",
            "Parsing annual reports of Cognizant Technology Solutions Corporation ...\n",
            "Parsing annual reports of Cablevision Systems Corporation ...\n",
            "Parsing annual reports of Cadence Design Systems ...\n",
            "Parsing annual reports of Caesars Entertainment ...\n",
            "Parsing annual reports of Camden Property Trust ...\n",
            "Parsing annual reports of Carrier Global ...\n",
            "Parsing annual reports of Catalent ...\n",
            "Parsing annual reports of Cboe Global Markets ...\n",
            "Parsing annual reports of CDW ...\n",
            "Parsing annual reports of Celanese ...\n",
            "Parsing annual reports of Centene Corporation ...\n",
            "Parsing annual reports of Ceridian ...\n",
            "Parsing annual reports of Charles River Laboratories ...\n",
            "Parsing annual reports of Charter Communications ...\n",
            "Parsing annual reports of Church & Dwight ...\n",
            "Parsing annual reports of Citizens Financial Group ...\n",
            "Parsing annual reports of Constellation Energy ...\n",
            "Parsing annual reports of CooperCompanies ...\n",
            "Parsing annual reports of Copart ...\n",
            "Parsing annual reports of Corteva ...\n",
            "Parsing annual reports of CoStar Group ...\n",
            "Parsing annual reports of CVS Health Corporation ...\n",
            "Parsing annual reports of Chevron Corporation ...\n",
            "Parsing annual reports of Dominion Resources_ Inc_ ...\n",
            "Parsing annual reports of Delta Air Lines_ Inc_ ...\n",
            "Parsing annual reports of Deere & Company ...\n",
            "Parsing annual reports of Discover Financial Services ...\n",
            "Parsing annual reports of Dollar General Corporation ...\n",
            "Parsing annual reports of Quest Diagnostics Inc_ ...\n",
            "Parsing annual reports of DR Horton Inc_ ...\n",
            "Parsing annual reports of Danaher Corp_ ...\n",
            "Parsing annual reports of Discovery Communications_ Inc_ ...\n",
            "Parsing annual reports of Delphi Automotive PLC ...\n",
            "Parsing annual reports of Dollar Tree_ Inc_ ...\n",
            "Parsing annual reports of Dover Corporation ...\n",
            "Parsing annual reports of Dr Pepper Snapple Group_ Inc_ ...\n",
            "Parsing annual reports of Darden Restaurants_ Inc_ ...\n",
            "Parsing annual reports of DTE Energy Company ...\n",
            "Parsing annual reports of Duke Energy Corporation ...\n",
            "Parsing annual reports of DaVita HealthCare Partners Inc_ ...\n",
            "Parsing annual reports of Devon Energy Corporation ...\n",
            "Parsing annual reports of Dexcom ...\n",
            "Parsing annual reports of Diamondback Energy ...\n",
            "Parsing annual reports of Digital Realty ...\n",
            "Parsing annual reports of Dish Network ...\n",
            "Parsing annual reports of Domino_s ...\n",
            "Parsing annual reports of Dow Inc_ ...\n",
            "Parsing annual reports of DuPont ...\n",
            "Your query was successful! Time elapsed:00:43:38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CPoMma0kNe31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffb6d53-83fd-4d91-efcc-28f6e9283617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}